1. DA with multiple sources.

2. DA with scarce data. Cconsider generative transfer learning. Double translation.

3. DA Federate learning. Not transmit data, but transmit the generative model trained on the local machine.  

4. Identify noise samples. Using clustering or its neighbor relationships or select some reliable points. Rather than only using small loss.

5. Update by disagreement for ensemble, for semi-supervised learning or DA. To increase the divergence. Two diverged networks have different abilities to filter different types of error. The optimal peer should be complementary, rather than the consistent. Two students are good at math, one student is good at math and the other one is good at literature.

6. 
